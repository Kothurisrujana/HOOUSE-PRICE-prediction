# -*- coding: utf-8 -*-
"""housepriceprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmb8nexo1FPLjfUH6D-NPoBxQZoqs4pG

**Problem Statement**

Predict house prices in Hyderabad using property features such as area, location, BHK, rate per sqft, and building status.

**1Ô∏è‚É£ Understand the Problem & Dataset**
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/Hyderbad_House_price.csv')

df.head()

df.shape

df.duplicated().sum()

df.info()

df.describe().T

df.corr

df.columns

df

"""**2Ô∏è‚É£ Data Cleaning**

**`1.Remove unnecessary columns`**
"""

df.drop(columns=['Unnamed: 0'], inplace=True)

"""**2.Check missing values**"""

df.isnull().sum()

"""**3Ô∏è‚É£ Exploratory Data Analysis (EDA)**

**To check skewness, outliers, and price range**
"""

df['price(L)'].describe()

import matplotlib.pyplot as plt
plt.figure()
plt.scatter(df['area_insqft'], df['price(L)'])
plt.xlabel("Area (sqft)")
plt.ylabel("Price (Lakhs)")
plt.title("Price vs Area")
plt.show()

"""**Insight: Moderate positive relationship ‚Üí bigger houses generally cost more.**"""

plt.figure()
plt.scatter(df['rate_persqft'], df['price(L)'])
plt.xlabel("Rate per Sqft")
plt.ylabel("Price (Lakhs)")
plt.title("Price vs Rate per Sqft")
plt.show()

"""**Insight: Strong positive relationship ‚Üí rate per sqft is a key price driver.**"""

df[['price(L)', 'area_insqft', 'rate_persqft']].corr()

plt.figure()
df.boxplot(column='price(L)', by='building_status')
plt.xlabel("Building Status")
plt.ylabel("Price (Lakhs)")
plt.title("Price by Building Status")
plt.suptitle("")
plt.show()

"""**Insight: Ready-to-move houses are priced higher than under-construction ones.**"""

top_locations = df['location'].value_counts().head(10).index
df_top = df[df['location'].isin(top_locations)]

plt.figure()
df_top.boxplot(column='price(L)', by='location', rot=45)
plt.xlabel("Location")
plt.ylabel("Price (Lakhs)")
plt.title("Price Distribution Across Top 10 Locations")
plt.suptitle("")
plt.show()

"""**Insight: Location causes significant price variation.**

**4Ô∏è‚É£ Outlier Detection & Handling**

Outliers are extreme values that can distort model learning, especially in regression.
If not handled:
---MAE increases
---Model becomes biased    toward  high prices
"""

num_cols = ['price(L)', 'area_insqft', 'rate_persqft']

"""**Visualize Outliers using Boxplots**"""

plt.figure()
df.boxplot(column='price(L)')
plt.title("Boxplot of House Prices")
plt.ylabel("Price (Lakhs)")
plt.show()

plt.figure()
df.boxplot(column='area_insqft')
plt.title("Boxplot of Area")
plt.ylabel("Area (sqft)")
plt.show()

plt.figure()
df.boxplot(column='rate_persqft')
plt.title("Boxplot of Rate per Sqft")
plt.ylabel("Rate per Sqft")
plt.show()

"""üè† Should We Remove All Outliers in House Price Data?
‚ùå Removing all outliers is NOT recommended
‚úÖ Selective & domain-aware handling is the correct approach **bold text**

‚úî Keeps luxury houses
‚úî Reduces extreme influence
"""

df['price(L)'] = df['price(L)'].clip(
    lower=df['price(L)'].quantile(0.01),
    upper=df['price(L)'].quantile(0.99)
)

"""Remove outliers only from training set and Log-transform the target variable"""

import numpy as np
y = np.log1p(df['price(L)'])

df.shape

"""Alternative Approach (Capping Instead of Removing) which is suitable because our dataset is small"""

upper_limit = df['price(L)'].quantile(0.99)
df['price(L)'] = df['price(L)'].clip(upper=upper_limit)

df

"""**5: Feature Engineering**:

Raw features don‚Äôt always capture real-world meaning.
Feature engineering helps the model learn better patterns.

Extract BHK from title:
title is text ‚Üí ML can‚Äôt understand it directly

BHK (2, 3, 4) is a strong numerical indicator of price
"""

df['bhk'] = df['title'].str.extract('(\d+)').fillna(0).astype(int)
df.drop(columns=['title'], inplace=True)

df['derived_rate'] = (df['price(L)'] * 100000) / df['area_insqft']

df

df.drop(columns=['derived_rate'], inplace=True)

df

"""**6: Scaling & Encoding**
Why Scaling?

Prevents numerical dominance

Needed for distance-based & linear models

Safe practice inside pipelines

Why Encoding?

ML models don‚Äôt understand text

OneHotEncoder avoids false ordering

‚úî handle_unknown='ignore' prevents test errors
"""

X = df.drop('price(L)', axis=1)
y = df['price(L)']
num_cols = ['area_insqft', 'rate_persqft', 'bhk']
cat_cols = ['location', 'building_status']

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

preprocess = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
])

"""**7: Model Training**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""Why Gradient Boosting?

 Handles non-linearity,
 Robust to feature interactions,and
 Works well with mixed features

Model Pipeline
"""

from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor

model = Pipeline([
    ('preprocess', preprocess),
    ('gbr', GradientBoostingRegressor(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=3,
        random_state=42
    ))
])

"""Train the Model"""

model.fit(X_train, y_train)

"""**Evaluation & Insights**

Predictions
"""

y_pred = model.predict(X_test)

"""Evaluation Metrics"""

from sklearn.metrics import r2_score, mean_absolute_error

print("R¬≤ Score:", r2_score(y_test, y_pred))
print("MAE (Lakhs):", mean_absolute_error(y_test, y_pred))